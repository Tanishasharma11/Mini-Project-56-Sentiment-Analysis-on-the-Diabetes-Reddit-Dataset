{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis \n- Private dataset with sensitive hospital comments ","metadata":{}},{"cell_type":"code","source":"!kaggle datasets download -d tanavbajaj/diabetes-subreddits-data-weekly-update","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport pandas as pd\nimport spacy\nimport en_core_web_sm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:30:07.992664Z","iopub.execute_input":"2023-06-18T19:30:07.993242Z","iopub.status.idle":"2023-06-18T19:30:24.626287Z","shell.execute_reply.started":"2023-06-18T19:30:07.993198Z","shell.execute_reply":"2023-06-18T19:30:24.62493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nnltk.download('wordnet2022')\n\n! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:30:33.037397Z","iopub.execute_input":"2023-06-18T19:30:33.037796Z","iopub.status.idle":"2023-06-18T19:30:35.092891Z","shell.execute_reply.started":"2023-06-18T19:30:33.037767Z","shell.execute_reply":"2023-06-18T19:30:35.091318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/private-hospital-comments/comments1.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:30:38.129197Z","iopub.execute_input":"2023-06-18T19:30:38.129659Z","iopub.status.idle":"2023-06-18T19:30:38.214856Z","shell.execute_reply.started":"2023-06-18T19:30:38.129623Z","shell.execute_reply":"2023-06-18T19:30:38.213962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing the Dataset ","metadata":{}},{"cell_type":"code","source":"df[\"Main Topic\"] = df[\"Main Topic\"].fillna(\"No Topic Given\")\ndf = df.dropna(subset=[\"Content\"])\nnan_counts = df.isna().sum()\n\nprint(nan_counts)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:30:44.465014Z","iopub.execute_input":"2023-06-18T19:30:44.465537Z","iopub.status.idle":"2023-06-18T19:30:44.512748Z","shell.execute_reply.started":"2023-06-18T19:30:44.465499Z","shell.execute_reply":"2023-06-18T19:30:44.51146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining regex patterns.\nlinebreaks        = \"<br /><br />\"\nalphaPattern      = \"[^a-z0-9<>]\"\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n# Defining regex for emojis\nsmileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\nsademoji          = r\"[8:=;]['`\\-]?\\(+\"\nneutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\nlolemoji          = r\"[8:=;]['`\\-]?p+\"\n\nstop_words = set(stopwords.words('english'))\nLemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:31:12.707738Z","iopub.execute_input":"2023-06-18T19:31:12.709116Z","iopub.status.idle":"2023-06-18T19:31:12.716709Z","shell.execute_reply.started":"2023-06-18T19:31:12.709074Z","shell.execute_reply":"2023-06-18T19:31:12.715707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_reviews(review):\n\n    review = review.lower()\n\n    review = re.sub(linebreaks,\" \",review)\n    # Replace 3 or more consecutive letters by 2 letter.\n    review = re.sub(sequencePattern, seqReplacePattern, review)\n\n    # Replace all emojis.\n    review = re.sub(r'<3', '<heart>', review)\n    review = re.sub(smileemoji, '<smile>', review)\n    review = re.sub(sademoji, '<sadface>', review)\n    review = re.sub(neutralemoji, '<neutralface>', review)\n    review = re.sub(lolemoji, '<lolface>', review)\n\n    # Remove non-alphanumeric and symbols\n    review = re.sub(alphaPattern, ' ', review)\n    \n    # Tokenize the input text\n    tokens = word_tokenize(review)\n    \n    # Remove stop words from the token sequence\n\n    tokens = [token for token in tokens if token not in stop_words]\n    \n    # Lemmatize the remaining tokens\n    tokens = [Lemmatizer.lemmatize(token) for token in tokens]\n    \n    # Join the cleaned tokens into a single string\n    return ' '.join(tokens)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:31:13.088038Z","iopub.execute_input":"2023-06-18T19:31:13.089475Z","iopub.status.idle":"2023-06-18T19:31:13.099343Z","shell.execute_reply.started":"2023-06-18T19:31:13.089427Z","shell.execute_reply":"2023-06-18T19:31:13.09787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine \"Main Topic\", \"Subtopic\", and \"Content\" columns into a single column called \"Text\"\ndf[\"Text\"] = df[\"Main Topic\"] + \" \" + df[\"Subtopic\"] + \" \" + df[\"Content\"]","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:31:16.400107Z","iopub.execute_input":"2023-06-18T19:31:16.401672Z","iopub.status.idle":"2023-06-18T19:31:16.421833Z","shell.execute_reply.started":"2023-06-18T19:31:16.401605Z","shell.execute_reply":"2023-06-18T19:31:16.420127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Text\"] = df[\"Text\"].apply(preprocess_reviews)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:31:18.494098Z","iopub.execute_input":"2023-06-18T19:31:18.494588Z","iopub.status.idle":"2023-06-18T19:31:26.295927Z","shell.execute_reply.started":"2023-06-18T19:31:18.494553Z","shell.execute_reply":"2023-06-18T19:31:26.294695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keyword and Sentiment Analysis using Word2Vec \n- Reference Article: https://towardsdatascience.com/unsupervised-semantic-sentiment-analysis-of-imdb-reviews-2c5f520fbf81","metadata":{}},{"cell_type":"code","source":"!curl https://raw.githubusercontent.com/towardsNLP/IMDB-Semantic-Sentiment-Analysis/main/Word2Vec/src/w2v_utils.py -o w2v_utils.py","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:23.425327Z","iopub.execute_input":"2023-06-18T19:34:23.425827Z","iopub.status.idle":"2023-06-18T19:34:25.869583Z","shell.execute_reply.started":"2023-06-18T19:34:23.425791Z","shell.execute_reply":"2023-06-18T19:34:25.868452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from w2v_utils import (Tokenizer,\n                       evaluate_model,\n                       bow_vectorizer,\n                       train_logistic_regressor,\n                       w2v_trainer,\n                       calculate_overall_similarity_score,\n                       overall_semantic_sentiment_analysis,\n                       list_similarity,\n                       calculate_topn_similarity_score,\n                       topn_semantic_sentiment_analysis,\n                       define_complexity_subjectivity_reviews,\n                       explore_high_complexity_reviews,\n                       explore_low_subjectivity_reviews,\n                       text_SSA)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:25.87231Z","iopub.execute_input":"2023-06-18T19:34:25.872693Z","iopub.status.idle":"2023-06-18T19:34:27.331902Z","shell.execute_reply.started":"2023-06-18T19:34:25.872661Z","shell.execute_reply":"2023-06-18T19:34:27.330557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Instancing the Tokenizer class\ntokenizer = Tokenizer(clean= True,\n                      lower= True, \n                      de_noise= True, \n                      remove_stop_words= True,\n                      keep_negation=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:30.746511Z","iopub.execute_input":"2023-06-18T19:34:30.746968Z","iopub.status.idle":"2023-06-18T19:34:30.754911Z","shell.execute_reply.started":"2023-06-18T19:34:30.746937Z","shell.execute_reply":"2023-06-18T19:34:30.753301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tokenized_text'] = df['Text'].apply(tokenizer.tokenize)\n\ndf['tokenized_text_len'] = df['tokenized_text'].apply(len)\ndf['tokenized_text_len'].apply(np.log).describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:32.828657Z","iopub.execute_input":"2023-06-18T19:34:32.829108Z","iopub.status.idle":"2023-06-18T19:34:33.742275Z","shell.execute_reply.started":"2023-06-18T19:34:32.829078Z","shell.execute_reply":"2023-06-18T19:34:33.741097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyed_vectors , keyed_vocab = w2v_trainer(df[\"tokenized_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:38.983321Z","iopub.execute_input":"2023-06-18T19:34:38.983854Z","iopub.status.idle":"2023-06-18T19:34:41.645314Z","shell.execute_reply.started":"2023-06-18T19:34:38.983796Z","shell.execute_reply":"2023-06-18T19:34:41.644004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(keyed_vectors))\nprint(type(keyed_vocab))","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:42.723907Z","iopub.execute_input":"2023-06-18T19:34:42.725406Z","iopub.status.idle":"2023-06-18T19:34:42.730921Z","shell.execute_reply.started":"2023-06-18T19:34:42.725357Z","shell.execute_reply":"2023-06-18T19:34:42.729701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyed_vectors.most_similar(\"research\",topn=15)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:44.848228Z","iopub.execute_input":"2023-06-18T19:34:44.848652Z","iopub.status.idle":"2023-06-18T19:34:44.880546Z","shell.execute_reply.started":"2023-06-18T19:34:44.848624Z","shell.execute_reply":"2023-06-18T19:34:44.878682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyed_vectors.most_similar(\"hospital\",topn=15)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:49.254794Z","iopub.execute_input":"2023-06-18T19:34:49.255279Z","iopub.status.idle":"2023-06-18T19:34:49.275791Z","shell.execute_reply.started":"2023-06-18T19:34:49.255244Z","shell.execute_reply":"2023-06-18T19:34:49.273886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyed_vectors.most_similar(\"funded\",topn=15)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:49.572073Z","iopub.execute_input":"2023-06-18T19:34:49.572496Z","iopub.status.idle":"2023-06-18T19:34:49.589402Z","shell.execute_reply.started":"2023-06-18T19:34:49.572459Z","shell.execute_reply":"2023-06-18T19:34:49.587596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering Approach to Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"# To make sure that all `positive_concepts` are in the keyed word2vec vocabulary\npositive_concepts = ['excellent', 'awesome', 'cool','decent','amazing', 'strong', 'good', 'great', 'funny', 'entertaining'] \npos_concepts = [concept for concept in positive_concepts if concept in keyed_vocab]","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:54.508288Z","iopub.execute_input":"2023-06-18T19:34:54.508781Z","iopub.status.idle":"2023-06-18T19:34:54.514775Z","shell.execute_reply.started":"2023-06-18T19:34:54.508743Z","shell.execute_reply":"2023-06-18T19:34:54.513872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To make sure that all `negative_concepts` are in the keyed word2vec vocabulary \nnegative_concepts = ['terrible','awful','horrible','boring','bad', 'disappointing', 'weak', 'poor',  'senseless','confusing'] \nneg_concepts = [concept for concept in negative_concepts if concept in keyed_vocab]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:54.869381Z","iopub.execute_input":"2023-06-18T19:34:54.870087Z","iopub.status.idle":"2023-06-18T19:34:54.875218Z","shell.execute_reply.started":"2023-06-18T19:34:54.870053Z","shell.execute_reply":"2023-06-18T19:34:54.874019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating Semantic Sentiment Scores by OSSA model\noverall_df_scores = overall_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n                                                   positive_target_tokens = pos_concepts, \n                                                   negative_target_tokens = neg_concepts,\n                                                   doc_tokens = df['tokenized_text'])\n\n# Calculating Semantic Sentiment Scores by TopSSA model\ntopn_df_scores = topn_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n                                                   positive_target_tokens = pos_concepts, \n                                                   negative_target_tokens = neg_concepts,\n                                                   doc_tokens = df['tokenized_text'],\n                                                     topn=30)\n\n\n# To store semantic sentiment store computed by OSSA model in df\ndf['overall_PSS'] = overall_df_scores[0] \ndf['overall_NSS'] = overall_df_scores[1] \ndf['overall_semantic_sentiment_score'] = overall_df_scores[2] \ndf['overall_semantic_sentiment_polarity'] = overall_df_scores[3]\n\n\n\n# To store semantic sentiment store computed by TopSSA model in df\ndf['topn_PSS'] = topn_df_scores[0] \ndf['topn_NSS'] = topn_df_scores[1] \ndf['topn_semantic_sentiment_score'] = topn_df_scores[2] \ndf['topn_semantic_sentiment_polarity'] = topn_df_scores[3]","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:34:55.957204Z","iopub.execute_input":"2023-06-18T19:34:55.958469Z","iopub.status.idle":"2023-06-18T19:35:04.205327Z","shell.execute_reply.started":"2023-06-18T19:34:55.958419Z","shell.execute_reply":"2023-06-18T19:35:04.204181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = keyed_vectors.index_to_key\nvectors = [keyed_vectors[word] for word in words]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:35:04.207369Z","iopub.execute_input":"2023-06-18T19:35:04.207689Z","iopub.status.idle":"2023-06-18T19:35:04.223254Z","shell.execute_reply.started":"2023-06-18T19:35:04.207663Z","shell.execute_reply":"2023-06-18T19:35:04.222263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nresult = pca.fit_transform(vectors)\n\n# Create a DataFrame with PCA results and words\npca_df = pd.DataFrame(result, columns=['x', 'y'])\npca_df['word'] = words\n","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:35:06.272749Z","iopub.execute_input":"2023-06-18T19:35:06.273568Z","iopub.status.idle":"2023-06-18T19:35:06.537157Z","shell.execute_reply.started":"2023-06-18T19:35:06.273517Z","shell.execute_reply":"2023-06-18T19:35:06.535419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\n\nfig = go.Figure(data=go.Scattergl(\n    x=pca_df['x'],\n    y=pca_df['y'],\n    mode='markers',\n    marker=dict(\n        colorscale='Viridis',\n        line_width=1\n    ),\n    text=pca_df['word'],\n    textposition=\"bottom center\"\n))\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:35:12.359696Z","iopub.execute_input":"2023-06-18T19:35:12.360207Z","iopub.status.idle":"2023-06-18T19:35:12.578223Z","shell.execute_reply.started":"2023-06-18T19:35:12.360156Z","shell.execute_reply":"2023-06-18T19:35:12.576069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_pos_filt = df['topn_semantic_sentiment_polarity'] == 1\nactual_neg_filt =  df['topn_semantic_sentiment_polarity'] == 0\n\n# filter positive and negative review based on Most Probable predicted 'y' or 'topn_semantic_sentiment_score' column\npredicted_pos_filt = df['topn_semantic_sentiment_polarity'] == 1\npredicted_neg_filt = df['topn_semantic_sentiment_polarity'] == 0\n\n\n\n# plotting Semantic Sentiment Score Position of Actual Negative Reviews \nplt.scatter(df['topn_NSS'][actual_neg_filt], \n         df['topn_PSS'][actual_neg_filt],  \n         label='Actual Negetive Reviews',\n           color='DarkRed',\n            alpha=0.4 , # set transparency of color\n            s=20 # set size of dots\n           )\n\n# plotting Semantic Sentiment Score Position of Actual Positive Reviews \nplt.scatter(df['topn_NSS'][actual_pos_filt], \n         df['topn_PSS'][actual_pos_filt],  \n         label='Actual Positive Reviews',\n       color='DarkGreen',\n            alpha=0.1, # set transparency of color\n            s=30 # set size of dots\n           )\n# naming the x & y axis\nplt.xlabel('Predicted Negative Labels')\nplt.ylabel('Predicted Positive Labels')","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:35:27.012902Z","iopub.execute_input":"2023-06-18T19:35:27.01338Z","iopub.status.idle":"2023-06-18T19:35:27.383845Z","shell.execute_reply.started":"2023-06-18T19:35:27.013345Z","shell.execute_reply":"2023-06-18T19:35:27.382247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis Using BERT","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:35:35.314457Z","iopub.execute_input":"2023-06-18T19:35:35.314915Z","iopub.status.idle":"2023-06-18T19:35:50.497772Z","shell.execute_reply.started":"2023-06-18T19:35:35.314882Z","shell.execute_reply":"2023-06-18T19:35:50.496282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# importing the pipeline module\nfrom transformers import pipeline\n \n# Downloading the sentiment analysis model\nSentimentClassifier = pipeline(\"sentiment-analysis\")\n\n# Downloading the sentiment analysis model\n# SentimentClassifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:36:11.542487Z","iopub.execute_input":"2023-06-18T19:36:11.542899Z","iopub.status.idle":"2023-06-18T19:36:16.356202Z","shell.execute_reply.started":"2023-06-18T19:36:11.542868Z","shell.execute_reply":"2023-06-18T19:36:16.355142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to call for the whole dataframe\ndef FunctionBERTSentiment(inpText):\n  return(SentimentClassifier(inpText)[0]['label'])","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:36:16.358795Z","iopub.execute_input":"2023-06-18T19:36:16.360277Z","iopub.status.idle":"2023-06-18T19:36:16.366894Z","shell.execute_reply.started":"2023-06-18T19:36:16.360225Z","shell.execute_reply":"2023-06-18T19:36:16.365623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['BERT_Sentiment']=df['Text'].apply(FunctionBERTSentiment)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:36:16.368978Z","iopub.execute_input":"2023-06-18T19:36:16.370216Z","iopub.status.idle":"2023-06-18T19:41:19.091565Z","shell.execute_reply.started":"2023-06-18T19:36:16.370142Z","shell.execute_reply":"2023-06-18T19:41:19.090276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to call for the whole dataframe\ndef FunctionBERTScore(inpText):\n  return(SentimentClassifier(inpText)[0]['score'])","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:46:12.227331Z","iopub.execute_input":"2023-06-18T19:46:12.227813Z","iopub.status.idle":"2023-06-18T19:46:12.234674Z","shell.execute_reply.started":"2023-06-18T19:46:12.22777Z","shell.execute_reply":"2023-06-18T19:46:12.233506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Score']=df['Text'].apply(FunctionBERTScore)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:46:12.236459Z","iopub.execute_input":"2023-06-18T19:46:12.236875Z","iopub.status.idle":"2023-06-18T19:51:01.698258Z","shell.execute_reply.started":"2023-06-18T19:46:12.23684Z","shell.execute_reply":"2023-06-18T19:51:01.69696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Content_BERT_Sentiment']=df['Content'].apply(FunctionBERTSentiment)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:51:01.700267Z","iopub.execute_input":"2023-06-18T19:51:01.701044Z","iopub.status.idle":"2023-06-18T19:56:17.459349Z","shell.execute_reply.started":"2023-06-18T19:51:01.701Z","shell.execute_reply":"2023-06-18T19:56:17.457931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.to_csv('bert_sentiment.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:56:17.461213Z","iopub.execute_input":"2023-06-18T19:56:17.46168Z","iopub.status.idle":"2023-06-18T19:56:17.46782Z","shell.execute_reply.started":"2023-06-18T19:56:17.461638Z","shell.execute_reply":"2023-06-18T19:56:17.466633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, subPlot =plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nfig.suptitle(\"Sentiment analysis of Content + Topic Text\")\n \n# Grouping the data\nGroupedData=df.groupby('BERT_Sentiment').size()\n \n# Creating the charts\nGroupedData.plot(kind='bar', ax=subPlot[0], color=['crimson', 'lightblue'])\nGroupedData.plot(kind='pie', ax=subPlot[1], colors=['crimson', 'lightblue'])","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:56:17.469518Z","iopub.execute_input":"2023-06-18T19:56:17.469965Z","iopub.status.idle":"2023-06-18T19:56:17.881077Z","shell.execute_reply.started":"2023-06-18T19:56:17.469927Z","shell.execute_reply":"2023-06-18T19:56:17.879872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, subPlot =plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nfig.suptitle(\"Sentiment analysis of Content Only\")\n \n# Grouping the data\nGroupedData=df.groupby('Content_BERT_Sentiment').size()\n \n# Creating the charts\nGroupedData.plot(kind='bar', ax=subPlot[0], color=['crimson', 'lightblue'])\nGroupedData.plot(kind='pie', ax=subPlot[1], colors=['crimson', 'lightblue'])","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:56:17.882669Z","iopub.execute_input":"2023-06-18T19:56:17.883593Z","iopub.status.idle":"2023-06-18T19:56:18.23276Z","shell.execute_reply.started":"2023-06-18T19:56:17.883557Z","shell.execute_reply":"2023-06-18T19:56:18.231502Z"},"trusted":true},"execution_count":null,"outputs":[]}]}